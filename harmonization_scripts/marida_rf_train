import logging
import os
import random
import sys
import time
from os.path import dirname as up

import numpy as np
import pandas as pd
from joblib import dump

sys.path.append(up(up(os.path.abspath(__file__))))
from model.random_forest.random_forest import get_random_forest

random.seed(0)
np.random.seed(0)

# PATHS
root_path = up(up(up(os.path.abspath(__file__))))
data_path = '/data/sushen/marinedebris/project'

dataset_index = 3 # 0 for original classes, 1 for five classes, 2 for seven classes

hdf_paths = ['dataset_trial_1_original_classes.h5', 'dataset_trial_2_five_classes.h5', 'dataset_trial_3_seven_classes.h5', 'dataset_trial_4_five_classes.h5']
hdf_path = os.path.join(data_path, hdf_paths[dataset_index])

cl_paths = ['rf_classifier_trial_1.joblib', 'rf_classifier_trial_2.joblib', 'rf_classifier_trial_3.joblib', 'rf_classifier_trial_4.joblib']
cl_path = os.path.join(data_path, cl_paths[dataset_index])

# Logging
logging_paths = ['evaluation_rf_trial_1.log', 'evaluation_rf_trial_2.log', 'evaluation_rf_trial_3.log', 'evaluation_rf_trial_4.log']
logging_path = os.path.join(data_path, logging_paths[dataset_index])

logging.basicConfig(filename=logging_path, filemode='w',level=logging.INFO, format='%(name)s - %(levelname)s - %(message)s')
logging.info('*'*10)
logging.info("The dataset used is located at: " +str(hdf_path))
logging.info("Classifier is saved at: " +str(cl_path))


###############################################################
# Training                                                    #
###############################################################

def main():

    # Rewrite rf_features arrays to only use bands, real one must be imported from assets.py
    rf_features = ["B1", "B2", "B3", "B4", "B5", "B6", "B7", "B8", "B8A", "B11", "B12", 
                "NDVI", "FAI", "FDI", "SI", "NDWI", "NRD", "NDMI", "BSI",
                "con", "dis", "homo", "ener", "cor", "asm"]
    
    # Load Spectral Signatures
    
    hdf = pd.HDFStore(hdf_path, mode = 'r') 
    df_train = hdf.select('train')
    hdf.close()
    
    # Calculate weights for each sample based on Confidence Level
    df_train['Weight'] = 1/df_train['Conf']
    

    # Remove rows with NaN and unknow labels values
    df_train = df_train.dropna()
    df_train = df_train.reset_index()

    # Keep selected features and transform to numpy array
    X_train = df_train[rf_features].values
    y_train = df_train['Class'].values
    weight_train = df_train['Weight'].values
    
    print('Number of Input features: ', X_train.shape[1])
    print('Train: ',X_train.shape[0])
    
    print('Training X shape: ',X_train.shape)
    print('Training y shape: ',y_train.shape)

    logging.info('Number of Input features: ' + str(X_train.shape[1]))
    logging.info('Train: ' + str(X_train.shape[0]))

    # Training
    print('Started training')
    logging.info('Started training')
    
    start_time = time.time()
    rf_classifier = get_random_forest()
    rf_classifier.fit(X_train, y_train, **dict(rf__sample_weight=weight_train))
    
    print("Training finished after %s seconds" % (time.time() - start_time))
    logging.info("Training finished after %s seconds" % (time.time() - start_time))
    
    dump(rf_classifier, cl_path)

    # Paths printed for verification purposes
    print('')
    print('The dataset index is:', dataset_index)
    print('The HDF file path is:', hdf_path)
    print('The RF classifier joblib path is:', cl_path)
    
if __name__ == "__main__":
    main()
